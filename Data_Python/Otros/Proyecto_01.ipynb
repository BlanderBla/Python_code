{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "premium-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mature-package",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_20224/3504888549.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP-LAP~1\\AppData\\Local\\Temp/ipykernel_20224/3504888549.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    archivo = random.sample(os.listdir(\"C:\\Users\\HP-Laptop\\Desktop\\INTELIGENCIA 2\\PARCIAL 3\\CORPUS\"),1)\u001b[0m\n\u001b[1;37m                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "archivo = random.sample(os.listdir(\"C:\\Users\\HP-Laptop\\Desktop\\INTELIGENCIA 2\\PARCIAL 3\\CORPUS\"),1)\n",
    "archivo = archivo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/CORPUS/\"+archivo,\"r\", encoding=\"utf8\") as entrada:\n",
    "    texto = entrada.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "separadores = [\"(\",\")\",\",\",\".\",\";\",\":\",\"\\\"\",\"¿\",\"?\",\"¡\",\"!\",\"--\",\"_\"]\n",
    "\n",
    "for separador in separadores:\n",
    "    texto = texto.replace(separador,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c57b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "# Se debe reiniciar el kernel después de la instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ebe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es importante revisar el contexto de nuestro corpus, porque en ocasiones es neecsario modificar algunas características\n",
    "# del tokenizador. Por ejemplo: aquí quitamos los guiones al inicio y al final.\n",
    "\n",
    "for i,token in enumerate(tokens):\n",
    "    if token.startswith(\"_\") or token.startswith(\"—\"):\n",
    "        tokens[i] = tokens[i][1:]\n",
    "    if token.endswith(\"_\") or token.endswith(\"—\"):\n",
    "        tokens[i] = tokens[i][:-1]\n",
    "texto = \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d10594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a4d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def tokenizar(texto):\n",
    "#    puntuacion = ''\n",
    " #   tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "  #  for i,token in enumerate(tokens):\n",
    "        if token.startswith(\"_\") or token.startswith(\"—\"):\n",
    "            tokens[i] = tokens[i][1:]\n",
    "        if token.endswith(\"_\") or token.endswith(\"—\"):\n",
    "            tokens[i] = tokens[i][:-1]\n",
    "    texto = \" \".join(tokens)\n",
    "    tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(texto):\n",
    "    puntuacion = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¿¡'\n",
    "    tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "    for i,token in enumerate(tokens):\n",
    "        tokens[i] = token.strip(puntuacion)\n",
    "    texto = \" \".join(tokens)\n",
    "    tokens = nltk.word_tokenize(texto,\"spanish\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = {}\n",
    "corpus = os.listdir(\"D:/CORPUS\")\n",
    "for archivo in corpus:\n",
    "    with open(\"D:/CORPUS/\"+archivo,\"r\", encoding=\"utf8\") as entrada:\n",
    "        texto = entrada.read()\n",
    "    tokens = tokenizar(texto)\n",
    "    vocabulario = set(tokens)\n",
    "    for palabra in vocabulario:\n",
    "        if palabra not in indice:\n",
    "            indice[palabra] = set()\n",
    "        indice[palabra].add(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice[\"México\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice[\"México\"].intersection(indice[\"tecnología\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57229807",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a291a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmizar(tokens):\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"spanish\")\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6396e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos un nuevo índice\n",
    "indice = {}\n",
    "corpus = os.listdir(\"D:/CORPUS\")\n",
    "for archivo in corpus:\n",
    "    with open(\"D:/CORPUS/\"+archivo,\"r\", encoding=\"utf8\") as entrada:\n",
    "        texto = entrada.read()\n",
    "        texto = texto.lower()\n",
    "    tokens = tokenizar(texto)\n",
    "    stems = stemmizar(tokens)\n",
    "    vocabulario = set(stems)\n",
    "    for entrada in vocabulario:\n",
    "        if entrada not in indice:\n",
    "            indice[entrada] = set()\n",
    "        indice[entrada].add(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir de aquí las palabras a buscar deben de ir con la función stemmizar\n",
    "indice[stemmizar([\"oreja\"])[0]]\n",
    "#  así no: indice[\"oreja\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d459340",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_frecuencias = {}\n",
    "corpus = os.listdir(\"D:/CORPUS\")\n",
    "for archivo in corpus:\n",
    "    with open(\"D:/CORPUS/\"+archivo,\"r\", encoding=\"utf8\") as entrada:\n",
    "        texto = entrada.read()\n",
    "        texto = texto.lower()\n",
    "    tokens = tokenizar(texto)\n",
    "    stems = stemmizar(tokens)\n",
    "    vocabulario = stems\n",
    "    for entrada in vocabulario:\n",
    "        if entrada not in indice_frecuencias:\n",
    "            indice_frecuencias[entrada] = {}\n",
    "        if archivo not in indice_frecuencias[entrada]: \n",
    "            indice_frecuencias[entrada][archivo]=0\n",
    "        indice_frecuencias[entrada][archivo]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = indice_frecuencias.keys()\n",
    "corpus = corpus # solo como recordatorio de que alli estan los archivos\n",
    "tabla_frecuencias = pd.DataFrame(0,index=vocabulario,columns=corpus)\n",
    "for entrada in indice_frecuencias:\n",
    "    for archivo in indice_frecuencias[entrada]:\n",
    "        tabla_frecuencias.loc[entrada,archivo] = indice_frecuencias[entrada][archivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92949ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = random.sample(os.listdir(\"d:/CORPUS\"),1)\n",
    "archivo = archivo[0]\n",
    "tabla_frecuencias[archivo].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "palabras_funcionales=nltk.corpus.stopwords.words(\"spanish\")\n",
    "print(palabras_funcionales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizador(texto):\n",
    "    tokens = tokenizar(texto)\n",
    "    stems = stemmizar(tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "lista_archivos = [\"D:/CORPUS/\"+archivo for archivo in corpus]\n",
    "vectorizador = CountVectorizer(input=\"filename\",analyzer=\"word\")\n",
    "#vectorizador = CountVectorizer(input=\"filename\",analyzer=\"word\",tokenizer=tokenizador)\n",
    "#vectorizador = CountVectorizer(input=\"filename\",analyzer=\"word\",tokenizer = tokenizador, stop_words=palabras_funcionales)\n",
    "matriz_frecuencias = vectorizador.fit_transform(lista_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_frecuencias.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382944ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = vectorizador.get_feature_names()\n",
    "corpus = corpus # solo como recordatorio de que alli estan los archivos\n",
    "tabla_frecuencias = pd.DataFrame(matriz_frecuencias.toarray(),index=corpus,columns=vocabulario)\n",
    "tabla_frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = random.sample(os.listdir(\"D:/CORPUS\"),1)\n",
    "archivo = archivo[0]\n",
    "tabla_frecuencias.loc[archivo].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc853fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = os.listdir(\"D:/CORPUS\")\n",
    "lista_archivos = [\"D:/CORPUS/\"+archivo for archivo in corpus]\n",
    "vectorizador = TfidfVectorizer(input=\"filename\",analyzer=\"word\",sublinear_tf=True)\n",
    "matriz_tfidf = vectorizador.fit_transform(lista_archivos)\n",
    "vocabulario = vectorizador.get_feature_names()\n",
    "tabla_tfidf = pd.DataFrame(matriz_tfidf.toarray(),index=corpus,columns=vocabulario)\n",
    "tabla_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d19efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = random.sample(os.listdir(\"D:/CORPUS\"),1)\n",
    "archivo = archivo[0]\n",
    "tabla_tfidf.loc[archivo].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_tfidf[[\"méxico\",\"tecnología\"]].sum(axis=1).sort_values(ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
